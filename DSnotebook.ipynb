{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".h1_cell, .just_text {\n",
       "    box-sizing: border-box;\n",
       "    padding-top:5px;\n",
       "    padding-bottom:5px;\n",
       "    font-family: \"Times New Roman\", Georgia, Serif;\n",
       "    font-size: 125%;\n",
       "    line-height: 22px; /* 5px +12px + 5px */\n",
       "    text-indent: 25px;\n",
       "    background-color: #fbfbea;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "hr { \n",
       "    display: block;\n",
       "    margin-top: 0.5em;\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "    border-style: inset;\n",
       "    border-width: 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project for CIS 399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Benjamin Martinson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing a dataset called 'Grammer and Product Reviews' downloaded from Kaggle.com. My goal is to predict the rating of a product based on the text review that is given. To do so I will use the Naive Bayes method. The rating for each review is 1-5, so the method will output a prediction with 5 classes. I will take the highest predicted class of each review and compare it to the actual rating to see how close the prediction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_table = pd.read_csv('/Users/owner/Downloads/GrammarandProductReviews.csv',\n",
    "                          encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function I'm defining takes a string sentence and seperates each word, removing punctuation and 'stop words' (words that are very common and shouldn't be used in the comparisons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_wrangler(sentence, swords, punctuation):\n",
    "    removed_tokes = []\n",
    "    wrangled = []\n",
    "    word_tokes = word_punct_tokenizer.tokenize(sentence)\n",
    "    word_tokes = [x.lower() for x in word_tokes]\n",
    "    for i in range(len(word_tokes)):\n",
    "        foundPunct = False\n",
    "        for punct in punctuation:\n",
    "            if punct in word_tokes[i]:\n",
    "                foundPunct = True\n",
    "        if word_tokes[i] in swords:\n",
    "            removed_tokes.append(word_tokes[i])\n",
    "        elif foundPunct == True:\n",
    "            removed_tokes.append(word_tokes[i])\n",
    "        else:\n",
    "            wrangled.append(word_tokes[i].encode(\"utf-8\"))\n",
    "\n",
    "    return (wrangled, removed_tokes)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of sentence_wrangler in action. I'm printing the review text followed by the list of words extracted from that text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love this album. it's very good. more to the hip hop side than her current pop sound.. SO HYPE! i listen to this everyday at the gym! i give it 5star rating all the way. her metaphors are just crazy.\n",
      "\n",
      "[b'love', b'album', b'good', b'hip', b'hop', b'side', b'current', b'pop', b'sound', b'hype', b'listen', b'everyday', b'gym', b'give', b'5star', b'rating', b'way', b'metaphors', b'crazy']\n",
      "==========\n",
      "Good flavor. This review was collected as part of a promotion.\n",
      "\n",
      "[b'good', b'flavor', b'review', b'collected', b'part', b'promotion']\n",
      "==========\n",
      "Good flavor.\n",
      "\n",
      "[b'good', b'flavor']\n",
      "==========\n",
      "I read through the reviews on here before looking in to buying one of the couples lubricants, and was ultimately disappointed that it didn't even live up to the reviews I had read. For starters, neither my boyfriend nor I could notice any sort of enhanced or 'captivating' sensation. What we did notice, however, was the messy consistency that was reminiscent of a more liquid-y vaseline. It was difficult to clean up, and was not a pleasant, especially since it lacked the 'captivating' sensation we had both been expecting. I'm disappointed that I paid as much as I did for a lube that I won't use again, when I could just use their normal personal lubricant for 1) less money and 2) less mess.\n",
      "\n",
      "[b'read', b'reviews', b'looking', b'buying', b'one', b'couples', b'lubricants', b'ultimately', b'disappointed', b'even', b'live', b'reviews', b'read', b'starters', b'neither', b'boyfriend', b'could', b'notice', b'sort', b'enhanced', b'captivating', b'sensation', b'notice', b'however', b'messy', b'consistency', b'reminiscent', b'liquid', b'vaseline', b'difficult', b'clean', b'pleasant', b'especially', b'since', b'lacked', b'captivating', b'sensation', b'expecting', b'disappointed', b'paid', b'much', b'lube', b'use', b'could', b'use', b'normal', b'personal', b'lubricant', b'1', b'less', b'money', b'2', b'less', b'mess']\n",
      "==========\n",
      "My husband bought this gel for us. The gel caused irritation and it felt like it was burning my skin. I wouldn't recommend this gel.\n",
      "\n",
      "[b'husband', b'bought', b'gel', b'us', b'gel', b'caused', b'irritation', b'felt', b'like', b'burning', b'skin', b'recommend', b'gel']\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = review_table.loc[i, 'reviews.text']\n",
    "    print(text+'\\n')\n",
    "    print(sentence_wrangler(text, swords, punctuation)[0])\n",
    "    print('='*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function produces a dictionary with keys of all words extracted from each review text, with corresponding values identifying the rating given for the review that the word was used in. For example bag['hello'] = [0,1,0,5,0] means that 'hello' is in 6 review texts, where 1 review gave a rating of 2, and 5 reviews gave a rating of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def all_words(table, swords, punct):\n",
    "    bag = {}\n",
    "    for i in range(len(table)):\n",
    "        #Some review texts are NaN so I'm checking for that here\n",
    "        if pd.isnull(table.loc[i, 'reviews.text']):\n",
    "            continue\n",
    "            \n",
    "        text = table.loc[i, 'reviews.text']     \n",
    "        rating = table.loc[i, 'reviews.rating']\n",
    "        sentence = sentence_wrangler(text, swords, punctuation)\n",
    "        sentence = sentence[0]\n",
    "        #rating is one more than the rating index\n",
    "        idx = rating-1\n",
    "        if i %5000 == 0:\n",
    "            print('did 5000')\n",
    "        for word in sentence:\n",
    "            if word.isalpha():\n",
    "                if word not in bag:\n",
    "                    bag[word] = [0,0,0,0,0]\n",
    "                bag[word][idx] += 1   \n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26337"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = all_words(review_table, swords, punctuation)\n",
    "len(bag_of_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive bayes method needs the total count of each review rating given. I'll collect this in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3701, 1833, 4369, 14598, 46543]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count = [0,0,0, 0, 0]\n",
    "for index, row in review_table.iterrows():\n",
    "    idx = row['reviews.rating'] - 1\n",
    "    total_count[idx] +=1\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive_bayes function I'm defining uses the numClasses variable to represent the number of classes in the prediction, but that will be 5 for my purposes. The function returns a list of 5 elements that represent the prediction for each rating, 1-5 (left to right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_bayes(raw_sentence, bag, counts, numClasses):\n",
    "    sentence = sentence_wrangler(raw_sentence, swords, punctuation)\n",
    "    sentence = sentence[0]\n",
    "    numerator = [1] * numClasses\n",
    "    casePercentage = [0] * numClasses\n",
    "    tableSize = 0\n",
    "    for i in range(numClasses):\n",
    "        tableSize += counts[i]\n",
    "  \n",
    "    for i in range(numClasses):\n",
    "        for word in sentence:\n",
    "            if word in bag:\n",
    "                numerator[i] *= (bag[word][i] / counts[i])\n",
    "            \n",
    "        casePercentage[i] = counts[i] / tableSize\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(numClasses):\n",
    "        predictions.append(numerator[i] * casePercentage[i])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the following code as a check to make sure I'm on the right track. I'm printing the prediction list, followed by the predicted rating (max of the list) and the actual rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 3.7154811459226627e-47]\n",
      "Rating Prediction = 5\n",
      "actual rating = 5\n",
      "============\n",
      "[5.898149682819661e-10, 2.5187986026742685e-08, 2.078700058067819e-07, 9.029366301808492e-07, 3.0493451735918935e-06]\n",
      "Rating Prediction = 5\n",
      "actual rating = 5\n",
      "============\n",
      "[2.8113535661845036e-05, 2.1770238788879918e-05, 7.065276804400496e-05, 0.00018073082982545665, 0.0005387804752337192]\n",
      "Rating Prediction = 5\n",
      "actual rating = 5\n",
      "============\n",
      "[5.437953728208661e-108, 0.0, 0.0, 0.0, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[1.7115749906034258e-19, 4.362577677240861e-23, 2.61094409743329e-25, 7.638665597932919e-27, 7.797542175922421e-28]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[1.5470875712768262e-37, 0.0, 0.0, 1.5001378859331859e-43, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[5.542780014621437e-33, 0.0, 0.0, 1.2004092276944343e-38, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[8.544887589944767e-32, 0.0, 0.0, 1.567397639132955e-39, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[3.030485187441979e-27, 0.0, 0.0, 0.0, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n",
      "[4.873252226545688e-34, 0.0, 0.0, 0.0, 0.0]\n",
      "Rating Prediction = 1\n",
      "actual rating = 1\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    predictions = naive_bayes(review_table.loc[i, 'reviews.text'], bag_of_words, total_count, 5)\n",
    "    print(predictions)\n",
    "    m = max(predictions)\n",
    "    print('Rating Prediction =', predictions.index(m)+1)\n",
    "    print('actual rating =', review_table.loc[i, 'reviews.rating'])\n",
    "    print('============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 10 predictions happen to be correct but let's try it for all the reviews now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n",
      "did 5000\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(len(review_table)):\n",
    "    if pd.isnull(review_table.loc[i, 'reviews.text']):\n",
    "        continue\n",
    "    if i%5000 == 0: print('did 5000')\n",
    "    pair = naive_bayes(review_table.loc[i, 'reviews.text'], bag_of_words, total_count, 5)\n",
    "    m = max(pair)\n",
    "    predictions.append(pair.index(m)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To efficiently compare each prediction to the actual rating, I will zip the two side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actuals = review_table['reviews.rating']\n",
    "zipped = list(zip(predictions, actuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres the first 50. As you can see some predictions are off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 5),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 3),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (1, 4),\n",
       " (5, 5),\n",
       " (4, 5),\n",
       " (5, 5),\n",
       " (1, 5),\n",
       " (5, 5),\n",
       " (1, 5),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (4, 5),\n",
       " (5, 5),\n",
       " (2, 4),\n",
       " (4, 5),\n",
       " (1, 5),\n",
       " (5, 5),\n",
       " (5, 1),\n",
       " (1, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 4),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 4)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the accuracy percentage for all the predictions is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6085621743416421"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(zipped)):\n",
    "    if zipped[i][0]==zipped[i][1]:\n",
    "        correct += 1\n",
    "1.0*correct/len(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60% doesn't look great, but given that there are 5 different ratings to choose from, and some of the reviews texts are too short or not descriptive enough to make an exact prediction, 60% is actually not bad in my opinion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that some of the predictions are only off by 1. I will now calculate the accuracy percentage, including predictions off by one at most one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8324602168708632"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(zipped)):\n",
    "    if abs(zipped[i][0] - zipped[i][1]) <= 1:\n",
    "        correct += 1\n",
    "1.0*correct/len(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that over 80% of predictions are very close to correct. Now let's see off by at most 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.911970145049993"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(zipped)):\n",
    "    if abs(zipped[i][0] - zipped[i][1]) <= 2:\n",
    "        correct += 1\n",
    "1.0*correct/len(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider this a success because the method is able to predict the general positivity of the review text, whether the reviewer will give a positive of negative rating, with over 90% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
